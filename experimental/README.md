# ローカルLLM翻訳実験：推論レベル別性能分析と実用指針

フランス語からスペイン語にポッドキャストの翻訳を行い、Gemini 2.5 Flashによる客観評価を実施。構造化出力と推論レベルが翻訳品質に与える影響を体系的に分析しました。

※ 推論レベルが高くなるほど、詳細なreasoningを行うように構造化出力で誘導。具体的なスキーマは各セクションで説明。

## 実験の背景と設計思想

### 構造化出力による推論制御の理論的背景

**基本仮説**: スキーマフィールド順序でモデルの処理順序を制御できる

構造化出力のスキーマ設計により、`reasoning_level`パラメーターで翻訳手法を動的に切り替える柔軟なシステムを実装。モデルの思考プロセスを制御し、段階的な品質向上を実現することを目指しました。

### 評価手法の変遷と重要な発見

#### 初期評価（Claude Codeによる評価）
| 手法 | スコア | 改善幅 | 主要効果 |
|:---|:---:|:---:|:---|
| レベル0 (直接翻訳) | 65点 | - | ベースライン |
| レベル1 (推論付き) | 85点 | +20点 | 思考の連鎖 |
| レベル2 (2段階翻訳) | 93点 | +28点 | 自己品質チェック |

詳細👉[comparison/README.md](comparison/README.md)

#### 評価項目を定めた評価（Gemini 2.5 Flashによる評価）
同じ出力をGemini 2.5 Flashの5項目評価で測定すると、全く異なる結果が判明：

**主観評価の問題点**:
- 評価項目を決めずに漠然と評価するため、評価の根拠が不明確

**評価者分離の優位性**:
- **5項目体系的評価**: 読みやすさ、流暢性、専門用語、文脈適応、情報完全性
- **複数回評価による信頼性**: 3回評価の中央値で評価ブレを排除

#### 設計思想の根本的転換

- **従来の仮説**: 複雑な推論システム → 高品質翻訳
- **客観評価による新発見**: 適切なモデル選択 → シンプルな直接出力 = 最高効率

## 評価システムの技術詳細

### evaluate_translation.py: 翻訳品質評価ツール

**概要**: Gemini 2.5 Flashを使用した5項目翻訳品質評価システム

- **評価基準**: 5項目各20点満点（計100点）👉[EVAL.md](EVAL.md)
  1. **読みやすさと理解しやすさ**: 目標言語読者の理解容易性
  2. **流暢さと自然さ**: ネイティブスピーカーにとっての自然性
  3. **専門用語の適切性**: 技術用語の正確性と一貫性
  4. **文脈適応性**: 原文の意図と文化的背景の適切な反映
  5. **情報の完全性**: 情報の欠落・追加なく簡潔明瞭な伝達
- **統計的信頼性**: 3回評価の中央値を使用（[aggregate_evaluations.py](aggregate_evaluations.py)）
- **評価結果**: [SCORES.txt](SCORES.txt)

**使用方法**:
```bash
python evaluate_translation.py --original 原文.txt --translation 翻訳文.txt \
  -m gemini-2.5-flash-latest -f French -t Spanish -o 評価結果.json
```

**出力形式**: 各項目の詳細な推論と点数、総合評価コメント

### aggregate_evaluations.py: 評価結果集約ツール

**概要**: 複数回評価の統計的集約により信頼性の高い品質測定を実現

**主な機能**:
- **3回評価の自動検出**: `ファイル名-{1,2,3}.json`パターンを認識
- **統計値計算**: 中央値、平均値、標準偏差を項目別・総合別に算出
- **信頼性向上**: 評価のブレを統計的に補正

**使用方法**:
```bash
# 詳細表示
python aggregate_evaluations.py evaluation-*-*.json --verbose

# 簡潔表示（中央値のみ）
python aggregate_evaluations.py evaluation-*-*.json
```

**統計的意義**: 単発評価の主観的ブレを3回評価の中央値で排除し、客観的品質測定を実現

### 実験の実行とスコア取得方法
翻訳と評価の実行は`batch.sh`で一斉処理され、最終的にスコア集約まで自動化されています：

```bash
# 翻訳と評価を一斉実行
sh batch.sh
```

`batch.sh`では各翻訳結果に対して自動的に3回評価を実行し、統計的信頼性を確保しています。

### 翻訳システムの構成
- [translate.py](translate.py): 構造化出力による5段階推論レベル
- [translate4.py](translate4.py): 非構造化直接翻訳
- [translate5.py](translate5.py): 非構造化簡略推論
- [translate6.py](translate6.py): 非構造化詳細推論

### 凡例
- 表中のボールドは行ごとの最高点
- (t): `--translated-context`オプション（履歴に翻訳文のみ提供）
- (nt): `--no-think`オプション（reasoningモデルでの無効化）
  - Ollamaの制約により構造化出力利用時はreasoningが無効化され、(nt)と同様の効果

## 推論レベル別システム設計と実験スコア
[translate.py](translate.py)の `-r` オプションで推論レベル指定（すべて構造化出力）

| モデル | 0 | 1 | 2 | 3 | 4 |
|:---|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 76 | **91** | 66 | 62 | 67 |
| **aya-expanse-8b** | **63** | 45 | 61 | 36 | 20 |
| **command-r7b** | **63** | 61 | 44 | 40 | 34 |
| **gemma2-9b** | **74** | 30 | 70 | 58 | 34 |
| **gemma3-12b** | **95** | 11 | 90 | 86 | 89 |
| **gemma3-27b** | **96** | 66 | 94 | 58 | 70 |
| **gemma3-4b** | **52** | 42 | 47 | **52** | 20 |
| **gemma3n-e4b** | 72 | 17 | **88** | 70 | 53 |
| **gpt-oss-120b** | 92 | 96 | 96 | **97** | 71 |
| **gpt-oss-20b** | 88 | **96** | 87 | 94 | 86 |
| **llama3.3** | **86** | **86** | 61 | 71 | 39 |
| **llama4-scout** | **75** | 13 | 53 | 36 | 19 |
| **mistral-small3.2** | **92** | 85 | 75 | 90 | 89 |
| **phi4** | 82 | 80 | 81 | **83** | 74 |
| **qwen3-14b** | 90 | 71 | **91** | 87 | 80 |
| **qwen3-30b** | 84 | **92** | 84 | 84 | 56 |
| **qwen3-32b** | **95** | 91 | 84 | 94 | 85 |
| **qwen3-4b** | **71** | 52 | 60 | 65 | 69 |

- **レベル0（直接翻訳）**: 最も安定。大型ほど高得点。履歴は10前後で頭打ち。
- **レベル1（推論付き翻訳）**: 構造化制約下で指示が複雑化し、多くのモデルで顕著に劣化。例外は一部の強力モデルのみ。
- **レベル2（2段階翻訳）**: 中小型の底上げに有効だが、大型や高得点モデルでは利得が小さい。
- **レベル3（推論付き2段階翻訳）**: レベル1とレベル2の統合だが、レベル2に比べて明確な改善が認められない
- **レベル4（推論付き2段階翻訳の分割）**: レベル3をクエリ分割実行するが、レベル3に比べて劣化傾向（コンテキスト分断の悪影響だと推測）

**結論**: 分析・運用の主軸は0と2。3と4は調査対象から除外。

### レベル0: 直接翻訳
**特徴**: 最もシンプルな翻訳方式
```python
class Translation(BaseModel):
    translation: str = Field(description=f"Direct translation from {args.from_lang} to {args.to_lang}")
```

[translate.py](translate.py)の `--history` オプションでコンテキストに含める履歴数を指定（省略時のデフォルト値は5）

- 0-20は2回測定して変動を調査（他の項目もこの程度の変動はあると考えられる）

| モデル | 0-05 | 0-10 | 0-15 | 0-20 | 0-20-a | 0-20-b | 0-25 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 77 | 94 | - | - | 90 | **96** | - |
| **aya-expanse-8b** | 70 | **79** | - | - | **79** | 67 | - |
| **command-r7b** | 61 | 59 | - | - | **64** | 45 | - |
| **gemma2-9b** | 77 | **79** | - | - | 71 | 77 | - |
| **gemma3-12b** | 89 | 91 | - | - | **93** | 72 | - |
| **gemma3-27b** | **100** | 97 | - | - | 96 | 97 | - |
| **gemma3-4b** | 69 | **74** | - | - | 68 | 70 | - |
| **gemma3n-e4b** | 53 | 63 | - | - | 83 | **87** | - |
| **gpt-oss-120b** | 86 | 95 | - | - | 92 | **97** | - |
| **gpt-oss-20b** | 87 | 85 | - | - | **92** | 85 | - |
| **llama3.3** | 82 | **89** | - | - | **89** | 85 | - |
| **llama4-scout** | 89 | **90** | - | - | 81 | 89 | - |
| **mistral-small3.2** | 91 | **97** | - | - | 95 | 88 | - |
| **phi4** | 80 | **92** | - | - | 79 | 79 | - |
| **qwen3-14b** | **91** | 86 | - | - | 84 | **91** | - |
| **qwen3-30b** | 87 | **90** | - | - | **90** | 86 | - |
| **qwen3-32b** | 93 | 85 | - | - | 85 | **95** | - |
| **qwen3-4b** | 60 | 55 | 61 | **70** | 56 | 62 | 45 |
| **qwen3-4b (t)** | 56 | 53 | 56 | **60** | - | - | 47 |

- **(t)**: 一貫した改善は確認できず。対訳形式の安定化に寄与。
- 履歴最適値はモデル依存。高性能モデルは5〜10で頭打ち、軽量は15〜20で底上げ。ただし分散が増える。
- 履歴増は遅延・コスト増を伴うため、効果が薄い場合は戻す。

**結論**:
- 既定は --history 10
- 必要時のみ 15/20 を短期検証。分散増・劣化があれば 10 に戻す
- 20超は原則非推奨
- --translated-context は原則非推奨（再現性が低い）

## コンテキスト履歴（--history）オプション詳細

### 概要
`--history N`オプションは過去N件の翻訳履歴をコンテキストとして提供し、対話の一貫性と翻訳品質を向上させます。

### 設定値と効果
- **デフォルト値**: `--history 5`（省略時）
- **推奨設定**: `--history 10`（中・低性能モデルで大幅改善）
- **履歴形式**: 原文と翻訳文の対訳形式でコンテキストに含める

### 実装詳細
```python
# 直前の5つ（またはN個）の翻訳結果をコンテキストとして追加
context_lines = []
if context_history:
    context_lines.append("Previous conversation context:")
    context_lines.append("")
    for ctx in context_history[-N:]:  # Nは--historyで指定
        context_lines.append(f"Original: {ctx['speaker']}: {ctx['original']}")
        context_lines.append(f"Translation: {ctx['speaker']}: {ctx['translation']}")
        context_lines.append("")
```

### モデル別効果
- **高性能モデル** (Gemma3 12B, Qwen3 14B): 改善効果限定的
  - 既に高い文脈理解力を持つため追加履歴の恩恵は小さい
- **中性能モデル** (Phi4 14B): **劇的改善（+12点）**
  - **優れたコンテキスト把握力**により過去の翻訳履歴から一貫性を学習
  - **翻訳の一貫性が大幅向上**し、高性能モデルを凌駕する性能を実現
- **低性能モデル** (Qwen3 4B): 顕著改善（+5点）
  - 履歴情報により翻訳品質が底上げされる

### 関連オプション
- `--translated-context`: 履歴に翻訳文のみを提供（対訳形式でなく）
  - 実験結果では一貫性がなく、通常の対訳形式が推奨

### レベル1: 推論付き翻訳
**特徴**: 5項目詳細推論（構文解析、文脈解釈、語彙選択、文化的配慮、翻訳根拠）
```python
class Translation(BaseModel):
    reasoning: str = Field(description="""Detailed translation reasoning process:
1. Syntactic analysis of the original text...
2. Contextual interpretation of speaker's intent...
3. Evaluation of translation options...
4. Consideration of cultural nuances...
5. Justification for final translation choices...""")
    translation: str = Field(description="Translation result")
```

| モデル | 1-05 | 1-10 | 1-15 | 1-20 | 1-25 |
|:---|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 84 | 87 | - | **91** | - |
| **aya-expanse-8b** | 59 | **70** | - | 39 | - |
| **command-r7b** | 51 | 23 | - | **52** | - |
| **gemma2-9b** | **64** | 46 | - | 40 | - |
| **gemma3-12b** | 2 | **5** | - | **5** | - |
| **gemma3-27b** | 75 | **98** | - | 64 | - |
| **gemma3-4b** | 42 | 34 | - | **54** | - |
| **gemma3n-e4b** | 55 | **56** | - | 39 | - |
| **gpt-oss-120b** | 83 | 85 | - | **94** | - |
| **gpt-oss-20b** | **94** | 93 | - | 92 | - |
| **llama3.3** | **92** | 80 | - | 86 | - |
| **llama4-scout** | **31** | 10 | - | 20 | - |
| **mistral-small3.2** | 92 | 86 | - | **94** | - |
| **phi4** | 8 | 15 | - | **69** | - |
| **qwen3-14b** | **85** | 82 | - | 82 | - |
| **qwen3-30b** | 90 | 86 | - | **96** | - |
| **qwen3-32b** | 53 | **58** | - | 19 | - |
| **qwen3-4b** | 50 | 41 | 51 | **69** | 60 |
| **qwen3-4b (t)** | 50 | 58 | 57 | 58 | **71** |

- **(t)**: 出力形式は安定化するが、得点向上は不安定。
- 履歴は劣化の緩和に留まり、レベル0超えは稀。**例**: aya-expanse-32b(1-20=91), phi4(1-20=69)。
- **高得点を維持できる例外**: gpt-oss-20b(1系), qwen3-30b(1-20), gemma3-27b(1-10)。
- **失敗が顕著**: gemma3-12b は出力崩壊。gemma3-4b, llama4-scout は指示過多で品質低下。

**結論**:
- 実運用は原則レベル0。レベル1は教育・思考可視化に限定
- 試す場合はモデル別に履歴を最適化（例: gpt-oss-20b=10, qwen3-30b=20, gemma3-27b=10）

### レベル2: 2段階翻訳
**特徴**: 直接翻訳後、推敲して翻訳文を生成する2段階翻訳
```python
class Translation(BaseModel):
    draft_translation: str = Field(description="First draft translation")
    quality_assessment: str = Field(description="Analyze translation for errors, mistranslations, language mixing...")
    improvement_suggestions: str = Field(description="Provide specific suggestions for improving quality")
    improved_translation: str = Field(description="Improved translation based on assessment")
```

| モデル | 2-05 | 2-10 | 2-15 | 2-20 | 2-25 |
|:---|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 61 | **80** | - | 64 | - |
| **aya-expanse-8b** | 71 | **72** | - | 42 | - |
| **command-r7b** | 31 | **48** | - | 40 | - |
| **gemma2-9b** | 65 | 79 | - | **82** | - |
| **gemma3-12b** | **90** | **90** | - | 82 | - |
| **gemma3-27b** | 90 | 92 | - | **96** | - |
| **gemma3-4b** | **51** | 41 | - | 41 | - |
| **gemma3n-e4b** | **90** | 78 | - | 65 | - |
| **gpt-oss-120b** | 84 | **92** | - | 90 | - |
| **gpt-oss-20b** | 89 | 90 | - | **94** | - |
| **llama3.3** | 55 | 80 | - | **88** | - |
| **llama4-scout** | **74** | 61 | - | 62 | - |
| **mistral-small3.2** | **87** | 82 | - | 86 | - |
| **phi4** | 82 | **94** | - | 77 | - |
| **qwen3-14b** | **92** | 89 | - | 88 | - |
| **qwen3-30b** | 87 | **91** | - | 77 | - |
| **qwen3-32b** | 86 | **93** | - | 85 | - |
| **qwen3-4b** | 61 | **76** | 62 | 66 | 59 |
| **qwen3-4b (t)** | **71** | 35 | 61 | 63 | 68 |

- 中〜小型の底上げに有効。phi4(2-10), gemma3-12b, qwen3-14b が安定。
- gemma3n-e4b は顕著に改善するが、履歴増で劣化が大きい。
- (t) は qwen3-4b で不安定（2-10で大幅悪化）。

**結論**: 一定の品質を得やすいが、レベル0を超えないなら速度面で0を優先。

### 翻訳改善効果の検証（レベル0 vs レベル2）

レベル0（直接翻訳）とレベル2（2段階翻訳）の性能比較により、2段階翻訳による改善効果を検証

| モデル | 0-05 | 0-10 | 0-20-a | 0-20-b | 2-05 | 2-10 | 2-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 77 | 94 | 90 | **96** | 61 | 80 | 64 |
| **aya-expanse-8b** | 70 | **79** | **79** | 67 | 71 | 72 | 42 |
| **command-r7b** | 61 | 59 | **64** | 45 | 31 | 48 | 40 |
| **gemma2-9b** | 77 | 79 | 71 | 77 | 65 | 79 | **82** |
| **gemma3-12b** | 89 | 91 | **93** | 72 | 90 | 90 | 82 |
| **gemma3-27b** | **100** | 97 | 96 | 97 | 90 | 92 | 96 |
| **gemma3-4b** | 69 | **74** | 68 | 70 | 51 | 41 | 41 |
| **gemma3n-e4b** | 53 | 63 | 83 | 87 | **90** | 78 | 65 |
| **gpt-oss-120b** | 86 | 95 | 92 | **97** | 84 | 92 | 90 |
| **gpt-oss-20b** | 87 | 85 | 92 | 85 | 89 | 90 | **94** |
| **llama3.3** | 82 | **89** | **89** | 85 | 55 | 80 | 88 |
| **llama4-scout** | 89 | **90** | 81 | 89 | 74 | 61 | 62 |
| **mistral-small3.2** | 91 | **97** | 95 | 88 | 87 | 82 | 86 |
| **phi4** | 80 | 92 | 79 | 79 | 82 | **94** | 77 |
| **qwen3-14b** | 91 | 86 | 84 | 91 | **92** | 89 | 88 |
| **qwen3-30b** | 87 | 90 | 90 | 86 | 87 | **91** | 77 |
| **qwen3-32b** | 93 | 85 | 85 | **95** | 86 | 93 | 85 |
| **qwen3-4b** | 60 | 55 | 56 | 62 | 61 | **76** | 66 |

- 既に高得点なモデル（qwen3-14b, gemma3-12b, phi4）はレベル2の利得が小さい。
- qwen3-4b は 2-10 で底上げ（ただし絶対値は低い）。
- gemma3-4b は2段階化でかえって混乱。
- gemma3n-e4b は 2-05 が最良だが長履歴で劣化。速度を重視するなら 0-20 が実用的。

**結論**: 差が曖昧ならレベル0（直接翻訳）を優先。

### レベル3: 推論付き2段階翻訳

**特徴**: レベル1の推論とレベル2の2段階翻訳を統合
```python
class Translation(BaseModel):
    reasoning: str = Field(description="Detailed translation reasoning process...")
    draft_translation: str = Field(description="First draft translation")
    quality_assessment: str = Field(description="Analyze the draft translation for errors...")
    improvement_suggestions: str = Field(description="Provide specific suggestions...")
    improved_translation: str = Field(description="Based on the quality assessment...")
```
- **処理**: 推論→下訳→品質評価→改善提案→最終翻訳
- **利点**: 最も詳細な処理、全プロセス可視化
- **用途**: 研究目的、品質分析
- **実験結果**: レベル2に比べて明確な改善が認められない

### レベル4: 推論付き2段階翻訳の分割

**特徴**: レベル3を2つのステージに分割実行
```python
# 第1段階
class FirstStageTranslation(BaseModel):
    reasoning: str = Field(description="Detailed translation reasoning process...")
    draft_translation: str = Field(description="First draft translation")

# 第2段階
class SecondStageTranslation(BaseModel):
    quality_assessment: str = Field(description="Analyze the draft translation for errors...")
    improvement_suggestions: str = Field(description="Provide specific suggestions...")
    improved_translation: str = Field(description="Based on the quality assessment...")
```
- **処理**: ステージ1（推論+下訳）→ステージ2（品質評価+改善）
- **利点**: 段階的制御、メモリ効率化
- **用途**: 大規模翻訳、実験的処理
- **実験結果**: レベル3に比べて劣化傾向（コンテキスト分断の悪影響）

## 複雑な推論の逆効果メカニズム

客観評価により判明した、複雑推論による品質悪化の原因：

### 1. 翻訳選択肢の増加による混乱
- 推論プロセスで複数の翻訳候補を検討
- 選択肢が増えることで決定が不安定化
- 結果として一貫性のない翻訳が生成

### 2. 一貫性よりも局所最適化の優先
- 各段階で最適化を図るが全体最適を見失う
- 部分的な改善が全体品質を損なう
- フェーズ間での情報ロスが発生

### 3. 複雑な思考プロセスによる判断の不安定化
- 推論が深くなるほど迷いが生じる
- 自己評価による混乱が品質低下を招く
- シンプルな直接翻訳の方が安定した結果

## translate4.py: 非構造化直接翻訳による構造化制約の検証

構造化出力（translate.py -r 0）と非構造化出力（translate4.py）の直接翻訳性能を比較し、構造化制約の影響を検証しました。

### 核心的な発見

**構造化出力vs非構造化出力の比較実験**により、モデル特性に応じた個別最適化の重要性が判明：

| 推論方式 | 構造化出力 | 非構造化出力 | 性能差（例：Gemma3 12B, h05） |
|:---|:---|:---|:---|
| **直接翻訳** | レベル0: 89点 | translate4: 79点 | **-10点** |

### 重要な結論

1. **構造化出力の効果はモデル依存**: 全般的悪影響は存在せず、モデル・履歴数の組み合わせに強く依存
2. **個別最適化の重要性**: 画一的判断を避け、モデル特性に応じた設定が必要
3. **Reasoning制御の価値**: reasoning処理の制御が構造化出力制約よりも性能に大きく影響
4. **実行時安定性の考慮**: 評価スコアと実際の安定性が一致しない場合があり、実用性重視の選択が重要

**最適化戦略**: 
- **構造化出力優位**: Gemma3 12B、Gemma3 4B、Qwen3 14B（3/7モデル、43%）
- **非構造化出力優位**: Gemma2 9B、Gemma3n E4B、Phi4（3/7モデル、43%）
- **高得点での均衡**: 90点以上では構造化・非構造化が拮抗（各50%）

### 直接翻訳における構造化出力の影響調査（レベル0 vs tr4）

[translate.py](translate.py) の `-r 0` オプションで構造化出力、[translate4.py](translate4.py) で非構造化出力

| モデル | 0-05 | 0-10 | 0-20-a | 0-20-b | tr4-05 | tr4-10 | tr4-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 77 | 94 | 90 | 96 | 94 | **97** | 92 |
| **aya-expanse-8b** | 70 | **79** | **79** | 67 | 68 | 52 | 74 |
| **command-r7b** | 61 | 59 | 64 | 45 | 64 | **72** | 53 |
| **gemma2-9b** | 77 | 79 | 71 | 77 | **80** | 65 | 65 |
| **gemma3-12b** | 89 | 91 | 93 | 72 | 79 | **94** | 90 |
| **gemma3-27b** | **100** | 97 | 96 | 97 | **100** | 98 | 99 |
| **gemma3-4b** | 69 | **74** | 68 | 70 | 25 | 58 | 71 |
| **gemma3n-e4b** | 53 | 63 | 83 | **87** | 77 | 84 | 86 |
| **gpt-oss-120b** | 86 | 95 | 92 | **97** | 92 | 94 | 92 |
| **gpt-oss-20b** | 87 | 85 | **92** | 85 | 91 | 86 | 83 |
| **llama3.3** | 82 | 89 | 89 | 85 | 90 | 91 | **96** |
| **llama4-scout** | 89 | **90** | 81 | 89 | 82 | 83 | 76 |
| **mistral-small3.2** | 91 | **97** | 95 | 88 | 95 | 88 | 89 |
| **phi4** | 80 | **92** | 79 | 79 | 84 | 89 | **92** |
| **qwen3-14b** | **91** | 86 | 84 | **91** | 79 | 77 | 75 |
| **qwen3-14b (nt)** | - | - | - | - | 81 | **90** | 84 |
| **qwen3-30b** | 87 | **90** | **90** | 86 | 39 | 31 | 19 |
| **qwen3-30b (nt)** | - | - | - | - | **93** | 84 | 70 |
| **qwen3-32b** | 93 | 85 | 85 | 95 | 87 | **96** | 83 |
| **qwen3-32b (nt)** | - | - | - | - | **91** | 82 | 74 |
| **qwen3-4b** | 60 | 55 | 56 | **62** | 52 | 54 | 42 |
| **qwen3-4b (nt)** | - | - | - | - | **71** | 58 | 56 |

- **(nt)**: tr4 では nt により明確に改善。Ollama制約で reasoning 無効化時はレベル0とほぼ同等。
- qwen3-30b は tr4 で暴走し失敗。構造化（レベル0）は安定高性能。tr4 では nt 必須。
- gemma3-4b は構造化が優位（ただし出力崩壊の再試行が必要なことあり）。
- gpt-oss-20b, phi4 は両方式で高水準。

**結論**: 同等性能ならパース安定性から構造化（レベル0）を優先。qwen3-30b は構造化必須。

## translate5.py: 自由記述式推論による構造化出力制約の検証

レベル1での壊滅的失敗（gemma3-12b: 11点）に対処するため、構造化出力の制約を除去した自由記述式推論実験を実施しました。

### 推論プロンプト

```
First, briefly analyze the text for:
1. Key vocabulary and expressions
2. Speaker's intent and tone
3. Cultural context and appropriate register

Then provide your final translation on the last line.
```

### 核心的な発見

**構造化 vs 非構造化推論の対比実験**により、同じ推論プロセスでも実装方法で劇的に性能が変わることを実証 (gemma3-12b)：

| 推論方式 | 構造化出力 | 非構造化出力 | 性能差 |
|:---|:---|:---|:---|
| **直接翻訳** | レベル0: 95点 | translate4: 79点 | **-16点** |
| **推論付き翻訳** | レベル1: 11点 | translate5: 93点 | **+82点** |

### 重要な結論

1. **構造化出力制約の有害性実証**: レベル1（11点）→ translate5（93点）の+82点改善
2. **推論プロセス自体の限界**: レベル0（95点）に対してtranslate5（93点）は-2点の軽微な劣化
3. **言語化によるオーバーヘッド**: 明示的分析が注意容量を圧迫し翻訳品質を阻害
4. **後付け説明方式の優位性**: 品質とのトレードオフを回避する設計原則を提示

**最終判断**: 翻訳タスクにおいては直接翻訳（レベル0）が最適解。推論プロセスは構造化制約下では有害、自由記述式では品質をほぼ維持するが、コストパフォーマンスで直接翻訳に劣る。

## translate6.py: 改良された自由記述式推論による比較実験

translate5.pyの推論プロンプトを改良し、レベル1との公平な比較を実現するバージョンです。

### 改造の目的

translate5.pyの初期実験では、推論内容がレベル1と異なっていたため、公平な比較ができませんでした。translate6.pyは以下の改良を実施：

1. **推論内容の統一**: translate.py -r 1と同じ5項目の詳細推論を実装
2. **翻訳選択肢の検討**: 重要語彙や慣用表現の翻訳オプション評価を追加
3. **根拠の明確化**: 最終的な翻訳選択の正当化プロセスを強化

### 改良された推論プロンプト

```
First, provide detailed translation reasoning covering:
1. Syntactic analysis of the original {args.from_lang} text (subject, predicate, object, modifiers, etc.)
2. Contextual interpretation of speaker's intent and emotional tone
3. Evaluation of {args.to_lang} translation options for key vocabulary and idiomatic expressions
4. Consideration of cultural nuances and appropriate register/politeness level
5. Justification for final {args.to_lang} translation choices and overall approach

Then provide your final translation on the last line.
```

### 実験の価値

translate6.pyにより、構造化出力制約の影響をより正確に測定し、レベル1との真の性能差を検証可能になります。特に「どのように翻訳するか」という視点を含む完全な推論プロセスの比較が実現されます。

### 推論付き翻訳における構造化出力の影響調査（レベル1 vs tr6）

[translate.py](translate.py) の `-r 1` オプションで構造化出力、[translate6.py](translate6.py) で非構造化出力

| モデル | 1-05 | 1-10 | 1-20 | tr6-05 | tr6-10 | tr6-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 84 | 87 | 91 | 84 | **97** | 85 |
| **aya-expanse-8b** | 59 | **70** | 39 | 41 | 52 | 68 |
| **command-r7b** | 51 | 23 | 52 | **57** | 28 | 25 |
| **gemma2-9b** | 64 | 46 | 40 | 48 | 46 | **68** |
| **gemma3-12b** | 2 | 5 | 5 | 87 | **93** | 91 |
| **gemma3-27b** | 75 | **98** | 64 | 57 | 96 | 90 |
| **gemma3-4b** | 42 | 34 | 54 | **64** | 61 | 48 |
| **gemma3n-e4b** | 55 | 56 | 39 | **74** | 73 | 68 |
| **gpt-oss-120b** | 83 | 85 | 94 | **96** | 90 | 84 |
| **gpt-oss-20b** | 94 | 93 | 92 | **97** | 83 | 91 |
| **llama3.3** | **92** | 80 | 86 | 87 | 80 | 91 |
| **llama4-scout** | 31 | 10 | 20 | 63 | 67 | **82** |
| **mistral-small3.2** | 92 | 86 | **94** | 80 | 30 | 78 |
| **phi4** | 8 | 15 | 69 | 78 | **89** | 79 |
| **qwen3-14b** | 85 | 82 | 82 | 74 | **87** | 70 |
| **qwen3-14b (nt)** | - | - | - | **96** | 83 | 82 |
| **qwen3-30b** | 90 | 86 | **96** | 5 | 0 | 0 |
| **qwen3-30b (nt)** | - | - | - | 82 | **91** | 80 |
| **qwen3-32b** | 53 | 58 | 19 | 95 | **96** | 78 |
| **qwen3-32b (nt)** | - | - | - | 77 | 78 | **83** |
| **qwen3-4b** | 50 | 41 | **69** | 55 | 47 | 66 |
| **qwen3-4b (nt)** | - | - | - | 54 | 49 | **65** |

- tr6 はレベル1より概ね高品質。特に gemma3-12b, phi4, gemma3-4b, gemma3n-e4b が劇的改善。
- qwen3-30b は tr6 非 nt で壊滅。nt を付ければ回復するが、構造化（1系）の方が安定高得点。
- gpt-oss-20b は tr6-05 で全実験最高（97）。

**結論**: 推論を使うなら tr6 を第一選択。ただし qwen3-30b は非構造化で暴走しやすく、構造化（1系）または nt が必須。

### 自由記述式推論比較（tr5 vs tr6）

[translate5.py](translate5.py) は簡略推論、[translate6.py](translate6.py) は詳細推論

| モデル | tr5-05 | tr5-10 | tr5-20 | tr6-05 | tr6-10 | tr6-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-32b** | 89 | 83 | 79 | 84 | **97** | 85 |
| **aya-expanse-8b** | 61 | 42 | 39 | 41 | 52 | **68** |
| **command-r7b** | 51 | **59** | 20 | 57 | 28 | 25 |
| **gemma2-9b** | **75** | 74 | 68 | 48 | 46 | 68 |
| **gemma3-12b** | **93** | 89 | 78 | 87 | **93** | 91 |
| **gemma3-27b** | 97 | **98** | 97 | 57 | 96 | 90 |
| **gemma3-4b** | **88** | 78 | 77 | 64 | 61 | 48 |
| **gemma3n-e4b** | **79** | 73 | 69 | 74 | 73 | 68 |
| **gpt-oss-120b** | 97 | 90 | **99** | 96 | 90 | 84 |
| **gpt-oss-20b** | 83 | 92 | 95 | **97** | 83 | 91 |
| **llama3.3** | 87 | 82 | **92** | 87 | 80 | 91 |
| **llama4-scout** | 75 | 67 | 69 | 63 | 67 | **82** |
| **mistral-small3.2** | **81** | 76 | 79 | 80 | 30 | 78 |
| **phi4** | 76 | 85 | **93** | 78 | 89 | 79 |
| **qwen3-14b** | 77 | 73 | 86 | 74 | **87** | 70 |
| **qwen3-14b (nt)** | 85 | 80 | 88 | **96** | 83 | 82 |
| **qwen3-30b** | **14** | 8 | 5 | 5 | 0 | 0 |
| **qwen3-30b (nt)** | 63 | 63 | 81 | 82 | **91** | 80 |
| **qwen3-32b** | 71 | 90 | 89 | 95 | **96** | 78 |
| **qwen3-32b (nt)** | **92** | 87 | 90 | 77 | 78 | 83 |
| **qwen3-4b** | 44 | 46 | 60 | 55 | 47 | **66** |
| **qwen3-4b (nt)** | 8 | 10 | 5 | 54 | 49 | **65** |

- tr5 はモデル間のばらつきが大きい。gpt-oss-20b は tr5-20=95 と堅調、tr6-05=97 が最高。
- qwen3-30b は tr5/tr6 とも非 nt で暴走。nt で回復するが、構造化の方が安定。
- 軽量（gemma3-4b, gemma2-9b）は tr5 で相対的に善戦するが、絶対値は実用域ギリギリ。

**結論**: tr5 の優位は限定的。実用は gpt-oss-20b、次点で一部軽量。安定運用は tr6 か構造化を推奨。

## モデル別実用設定一覧

各モデルの上位3項目（90点以上）または最高点1項目をリストアップ。

| モデル | スコア | 設定 |
|:---|:---:|:---|
| **gemma3-27b** | 100 | 0-05, tr4-05 |
| **gemma3-27b** | 99 | tr4-20 |
| **gemma3-27b** | 98 | 1-10, tr4-10, tr5-10 |
| **gpt-oss-120b** | 99 | tr5-20 |
| **gpt-oss-120b** | 97 | 0-20-b, 3, tr5-05 |
| **gpt-oss-120b** | 96 | 1, 2, tr6-05 |
| **aya-expanse-32b** | 97 | tr4-10, tr6-10 |
| **aya-expanse-32b** | 96 | 0-20-b |
| **aya-expanse-32b** | 94 | 0-10, tr4-05 |
| **gpt-oss-20b** | 97 | tr6-05 |
| **gpt-oss-20b** | 96 | 1 |
| **gpt-oss-20b** | 95 | tr5-20 |
| **mistral-small3.2** | 97 | 0-10 |
| **mistral-small3.2** | 95 | 0-20-a, tr4-05 |
| **mistral-small3.2** | 94 | 1-20 |
| **llama3.3** | 96 | tr4-20 |
| **llama3.3** | 92 | 1-05, tr5-20 |
| **llama3.3** | 91 | tr4-10, tr6-20 |
| **qwen3-14b (nt)** | 96 | tr6-nt-05 |
| **qwen3-14b** | 92 | 2-05 |
| **qwen3-14b** | 91 | 0-05, 0-20-b, 2 |
| **qwen3-30b** | 96 | 1-20 |
| **qwen3-30b (nt)** | 93 | tr4-nt-05 |
| **qwen3-30b** | 92 | 1 |
| **qwen3-32b** | 96 | tr4-10, tr6-10 |
| **qwen3-32b** | 95 | 0, 0-20-b, tr6-05 |
| **qwen3-32b** | 94 | 3 |
| **gemma3-12b** | 95 | 0 |
| **gemma3-12b** | 94 | tr4-10 |
| **gemma3-12b** | 93 | 0-20-a, tr5-05, tr6-10 |
| **phi4** | 94 | 2-10 |
| **phi4** | 93 | tr5-20 |
| **phi4** | 92 | 0-10, tr4-20 |
| **gemma3n-e4b** | 90 | 2-05 |
| **llama4-scout** | 90 | 0-10 |
| **gemma3-4b** | 88 | tr5-05 |
| **gemma2-9b** | 82 | 2-20 |
| **aya-expanse-8b** | 79 | 0-10, 0-20-a |
| **qwen3-4b** | 76 | 2-10 |
| **command-r7b** | 72 | tr4-10 |

※ 85 点未満は実用には不向きです。

## 推奨モデルと設定

**注意**: Gemma と Llama は生成物にライセンス上の制約があります。Aya Expanse は商用利用できません。

### CPU 実行環境

1. **gemma3n-e4b**:
   - **2-05**: 教育利用

2. **gemma3-4b**:
   - **tr5-05**: 語彙面がやや弱い印象

※ メモリが 32GB 以上ある場合は gpt-oss-20b も選択肢。

### GPU 実行環境

1. **gemma3-27b**
   - **0-05, tr4-05**: 速度優先、品質優先（最高得点）
   - **tr4-10/20**: コンテキストの影響はほとんどない

2. **gpt-oss-120b**（モデルが巨大なため動作環境を選ぶ）
   - **0-20**: 速度優先
   - **2-05, tr6-05**: 教育利用
   - **tr5-05/20**: 今回の測定では好成績だが、変動がありtr6と同程度と思われるため非推奨

3. **mistral-small3.2**
    - **0-10**: モデルがやや大きいためあまり速くない

4. **aya-expanse-32b**
   - **tr4-10, 0-20**: モデルがやや大きいため速くない
   - **tr6-10**: 教育利用

5. **gpt-oss-20b**
   - **tr6-05**: 品質優先、教育利用

6. **qwen3-30b**
   - **1-20**: 推敲のため遅い

7. **qwen3-14b**
   - **tr6-nt-05**: 教育利用

8. **qwen3-32b**
   - **tr4-10**: モデルがやや大きく reasoning があるため遅い
   - **tr6-10**: 教育利用

9. **llama3.3**
   - **tr4-20**: モデルが巨大なため遅い

**教育利用**: レベル 2 と tr6 の 5 項目推論（構文解析、文脈解釈、語彙選択、文化的配慮、翻訳根拠）は人間の翻訳思考プロセスに近く、語学学習に有用。

## まとめ

### 構造化出力の特性
1. **最低性能要件**: gemma3-4bでは出力が頻繁に破綻
2. **複雑指示の限界**: レベル1は多くのモデルで処理能力を超過
3. **Ollamaの制約活用**: 構造化出力時のreasoning自動無効化が有効に作用

### 最適化戦略
1. **高性能モデル**: レベル0（直接翻訳）で最高品質
2. **中性能モデル**: レベル2 + 適切な履歴設定で大幅改善
3. **低性能モデル**: 非構造化出力の活用を検討

### オプション効果の総括
**--translated-context (t)**:
- 効果が限定的かつ不安定、非推奨

**--no-think (nt)**:
- qwen3系で大幅な性能向上
- Reasoningモデルでの翻訳タスクには有効

## 実用システム構築の指針

### モデル選択の優先順位
1. **Qwenシリーズ**: 活発な開発、安定した品質
2. **Phiシリーズ**: 優れたコンテキスト活用能力
3. **Gemmaシリーズ**: ライセンス制約を考慮して選択

### パフォーマンス vs 品質のバランス
- **明確な差異がない場合**: 処理速度優先でレベル0を選択
- **品質重視**: モデル特性に応じた個別最適化
- **教育用途**: 推論プロセス可視化のためtr6を活用

### 実装時の注意点
1. **スコア変動の考慮**: 単一評価でなく複数回測定を推奨
2. **安定性重視**: 評価スコアと実際の運用安定性は必ずしも一致しない
3. **リソース効率**: 複雑なシステムが常に高品質とは限らない

## 結論

本実験により、「複雑な推論システム = 高品質翻訳」という従来仮説は客観評価により否定され、「適切なモデル選択 + シンプルな直接出力 = 最高効率」が実証されました。

構造化出力による推論制御は教育的価値は高いものの、実用翻訳においては：
- **高性能モデル**: シンプルな直接翻訳が最適
- **中・低性能モデル**: 適切なコンテキスト履歴活用により高性能モデルを凌駕可能

これらの知見は翻訳以外の言語タスク（文章要約、コード生成、創作支援）にも応用可能ですが、常に**シンプルな直接出力の有効性**を検証することが重要です。

## その他の実験ツール一覧

以下のPythonスクリプトは研究開発過程で作成された実験的ツールです。詳細な実装内容と評価結果については [OBSOLETE.md](OBSOLETE.md) を参照してください。

- **translate-exp.py**: サブコマンド方式の多段階翻訳システム（Phase 1/2/2a/3対応）
- **translate2.py**: 3段階多モデル翻訳 一気通貫版
- **translate3.py**: Phase 2a統合システム 一気通貫版（推奨）
- **draft_to_text.py**: JSON形式中間データからテキスト抽出ユーティリティ
- **analyze_2stage_diff.py**: 翻訳手法別品質差分析ツール

これらのツールは実験的性質が強く、客観的評価によりレベル0（直接翻訳）の圧倒的優位性が判明したため、研究記録として保持されています。
