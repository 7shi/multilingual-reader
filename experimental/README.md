# ローカルLLM翻訳実験：推論レベル別性能分析と実用指針

フランス語からスペイン語にポッドキャストの翻訳を行い、GPT-OSS 120Bによる客観評価を実施。構造化出力と推論レベルが翻訳品質に与える影響を体系的に分析しました。

※ 推論レベルが高くなるほど、詳細なreasoningを行うように構造化出力で誘導。具体的なスキーマは各セクションで説明。

## 実験の背景と設計思想

### 構造化出力による推論制御の理論的背景

**基本仮説**: スキーマフィールド順序でモデルの処理順序を制御できる

構造化出力のスキーマ設計により、`reasoning_level`パラメーターで翻訳手法を動的に切り替える柔軟なシステムを実装。モデルの思考プロセスを制御し、段階的な品質向上を実現することを目指しました。

### 評価手法の変遷

#### 初期評価（Claude Codeによる評価）
| 手法 | スコア | 改善幅 | 主要効果 |
|:---|:---:|:---:|:---|
| レベル0 (直接翻訳) | 65点 | - | ベースライン |
| レベル1 (推論付き) | 85点 | +20点 | 思考の連鎖 |
| レベル2 (2段階翻訳) | 93点 | +28点 | 自己品質チェック |

詳細👉[comparison/README.md](comparison/README.md)

**問題点**: 評価項目を決めずに漠然と評価するため、評価の根拠が不明確

#### 評価項目を定めた評価
同じ出力を5項目評価で測定：

- **5項目体系的評価**: 読みやすさ、流暢性、専門用語、文脈適応、情報完全性
- **複数回評価による信頼性**: 3回評価の中央値で評価ブレを排除

**初期評価と異なる結果が判明**:
- **従来の仮説**: 複雑な推論システム → 高品質翻訳
- **客観評価による新発見**: 適切なモデル選択 → シンプルな直接出力 = 最高効率

#### 評価者間比較分析（Gemini 2.5 Flash vs GPT-OSS）
有償のGemini 2.5 Flashから無償のGPT-OSSへの移行可否を統計的に判定：

- **3評価者の比較**: Gemini 2.5 Flash、GPT-OSS 20B、GPT-OSS 120B（716項目）
- **統計分析**: 相関係数、一致度、系統的バイアス、問題ケースの抽出

詳細👉[evaluator_comparison/README.md](evaluator_comparison/README.md)

## 評価システムの技術詳細

### evaluate_translation.py: 翻訳品質評価ツール

**概要**: LLMを使用した5項目翻訳品質評価システム

- **評価基準**: 5項目各20点満点（計100点）👉[EVAL.md](EVAL.md)
  1. **読みやすさと理解しやすさ**: 目標言語読者の理解容易性
  2. **流暢さと自然さ**: ネイティブスピーカーにとっての自然性
  3. **専門用語の適切性**: 技術用語の正確性と一貫性
  4. **文脈適応性**: 原文の意図と文化的背景の適切な反映
  5. **情報の完全性**: 情報の欠落・追加なく簡潔明瞭な伝達
- **統計的信頼性**: 3回評価の中央値を使用（[aggregate_evaluations.py](aggregate_evaluations.py)）
- **評価結果**: [SCORES.txt](SCORES.txt)

**使用方法**:
```bash
python evaluate_translation.py --original 原文.txt --translation 翻訳文.txt \
  -m ollama:gpt-oss:120b -f French -t Spanish -o 評価結果.json
```

**出力形式**: 各項目の詳細な推論と点数、総合評価コメント

### aggregate_evaluations.py: 評価結果集約ツール

**概要**: 複数回評価の統計的集約により信頼性の高い品質測定を実現

**主な機能**:
- **3回評価の自動検出**: `ファイル名-{1,2,3}.json`パターンを認識
- **統計値計算**: 中央値、平均値、標準偏差を項目別・総合別に算出
- **信頼性向上**: 評価のブレを統計的に補正

**使用方法**:
```bash
# 詳細表示
python aggregate_evaluations.py evaluation-*-*.json --verbose

# 簡潔表示（中央値のみ）
python aggregate_evaluations.py evaluation-*-*.json
```

**統計的意義**: 単発評価の主観的ブレを3回評価の中央値で排除し、客観的品質測定を実現

### 実験の実行とスコア取得方法
翻訳と評価の実行は`batch.sh`で一斉処理され、最終的にスコア集約まで自動化されています：

```bash
sh batch.sh
```

各翻訳結果に対して自動的に3回評価を実行し、統計的信頼性を確保します。

**評価後の集計フロー**:
- `aggregate_evaluations.py`: 最終的なスコアを集約し、`SCORES.txt`に保存
- `generate_scores_md.py`: `SCORES.txt`から`SCORES.md`を生成
- `sync_scores.py`: `SCORES.md`の表を`README.md`に自動同期

### 翻訳システムの構成
- [translate.py](translate.py): 構造化出力による5段階推論レベル
- [translate4.py](translate4.py): 非構造化直接翻訳
- [translate5.py](translate5.py): 非構造化簡略推論
- [translate6.py](translate6.py): 非構造化詳細推論

### 凡例
- 表中のボールドは行ごとの最高点
- (t): `--translated-context`オプション（履歴に翻訳文のみ提供）
- (nt): `--no-think`オプション（reasoningモデルでの無効化）
  - Ollamaの制約により構造化出力利用時はreasoningが無効化され、(nt)と同様の効果

## 推論レベル別システム設計と実験スコア
[translate.py](translate.py)の `-r` オプションで推論レベル指定（すべて構造化出力）

| モデル | 0 | 1 | 2 | 3 | 4 |
|:---|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | **84** | 83 | 83 | 78 | 67 |
| **aya-expanse-32b** | 87 | **89** | 87 | 80 | 78 |
| **command-r7b** | **80** | 65 | 70 | 68 | 71 |
| **command-r-35b** | **88** | 82 | 71 | 75 | 80 |
| **gemma2-9b** | 80 | 71 | 79 | **87** | 71 |
| **gemma3-4b** | 79 | 41 | 65 | **81** | 30 |
| **gemma3-12b** | 84 | 24 | 85 | 79 | **90** |
| **gemma3-27b** | **92** | 84 | 85 | 82 | 78 |
| **gemma3n-e4b** | 86 | 66 | 84 | 82 | **87** |
| **gpt-oss-20b** | 85 | 87 | 87 | 90 | **91** |
| **gpt-oss-120b** | **92** | 89 | **92** | **92** | 86 |
| **llama3.3** | 82 | **90** | 70 | 80 | 70 |
| **llama4-scout** | **89** | 49 | 87 | 62 | 44 |
| **ministral-3-3b** | **84** | 20 | 43 | 67 | 40 |
| **ministral-3-8b** | **89** | 51 | 87 | 64 | 43 |
| **ministral-3-14b** | **89** | 80 | 74 | 26 | 21 |
| **mistral-small3.2** | 88 | 87 | 88 | **91** | 84 |
| **mixtral-8x7b** | **83** | 59 | 80 | 66 | 61 |
| **mixtral-8x22b** | **84** | 73 | 76 | 81 | 75 |
| **phi4** | 88 | 82 | 87 | **89** | 85 |
| **qwen3-4b** | 79 | 78 | 84 | 83 | **87** |
| **qwen3-4b (nt)** | 82 | 79 | **83** | 82 | 82 |
| **qwen3-14b** | **91** | 83 | **91** | 89 | 87 |
| **qwen3-14b (nt)** | **87** | 81 | **87** | 79 | 85 |
| **qwen3-30b** | 86 | 82 | 89 | 84 | **90** |
| **qwen3-30b (nt)** | 86 | **92** | 84 | 87 | 51 |
| **qwen3-32b** | **90** | 78 | 81 | 87 | 88 |
| **qwen3-32b (nt)** | 85 | 91 | **92** | 89 | 85 |

- **レベル0（直接翻訳）**: 最も安定。大型ほど高得点。履歴は10前後で頭打ち。
- **レベル1（推論付き翻訳）**: 構造化制約下で指示が複雑化し、多くのモデルで顕著に劣化。例外は一部の強力モデルのみ。
- **レベル2（2段階翻訳）**: 中小型の底上げに有効だが、大型や高得点モデルでは利得が小さい。
- **レベル3（推論付き2段階翻訳）**: レベル1とレベル2の統合だが、レベル2に比べて明確な改善が認められない
- **レベル4（推論付き2段階翻訳の分割）**: レベル3をクエリ分割実行するが、レベル3に比べて劣化傾向（コンテキスト分断の悪影響だと推測）

**結論**: 分析・運用の主軸は0と2。3と4は調査対象から除外。

### レベル0: 直接翻訳
**特徴**: 最もシンプルな翻訳方式
```python
class Translation(BaseModel):
    translation: str = Field(description=f"Direct translation from {args.from_lang} to {args.to_lang}")
```

[translate.py](translate.py)の `--history` オプションでコンテキストに含める履歴数を指定（省略時のデフォルト値は5）

- 0-20は2回測定して変動を調査（他の項目もこの程度の変動はあると考えられる）

| モデル | 0-05 | 0-10 | 0-15 | 0-20 | 0-20-a | 0-20-b | 0-25 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | 83 | **87** | - | - | 85 | 80 | - |
| **aya-expanse-32b** | 85 | **92** | - | - | 91 | **92** | - |
| **command-r7b** | 86 | 75 | - | - | **87** | 73 | - |
| **command-r-35b** | **91** | 90 | - | - | 81 | 90 | - |
| **gemma2-9b** | 78 | 76 | - | - | 78 | **87** | - |
| **gemma3-4b** | 75 | **87** | - | - | 86 | 83 | - |
| **gemma3-12b** | **91** | 90 | - | - | 88 | 90 | - |
| **gemma3-27b** | **92** | **92** | - | - | 91 | **92** | - |
| **gemma3n-e4b** | 86 | **91** | - | - | **91** | 89 | - |
| **gpt-oss-20b** | 86 | 81 | - | - | **89** | **89** | - |
| **gpt-oss-120b** | **91** | 89 | - | - | **91** | 88 | - |
| **llama3.3** | **85** | 82 | - | - | 82 | **85** | - |
| **llama4-scout** | 85 | 88 | - | - | 91 | **92** | - |
| **ministral-3-3b** | 83 | 77 | - | - | **88** | 81 | - |
| **ministral-3-8b** | 91 | **92** | - | - | 90 | 91 | - |
| **ministral-3-14b** | **90** | 85 | - | - | 86 | 86 | - |
| **mistral-small3.2** | **91** | 88 | - | - | 89 | **91** | - |
| **mixtral-8x7b** | 81 | **88** | - | - | 81 | 80 | - |
| **mixtral-8x22b** | 85 | 85 | - | - | 85 | **92** | - |
| **phi4** | 85 | **90** | - | - | 89 | 88 | - |
| **qwen3-4b** | 75 | **88** | 72 | 79 | 84 | 81 | 75 |
| **qwen3-4b (nt)** | 81 | 77 | 73 | **84** | 76 | 83 | 74 |
| **qwen3-4b (t)** | 76 | 75 | **82** | 72 | - | - | 75 |
| **qwen3-4b (nt,t)** | 72 | 73 | 75 | **79** | - | - | 70 |
| **qwen3-14b** | 87 | 78 | - | - | 83 | **88** | - |
| **qwen3-14b (nt)** | 85 | 86 | - | - | 85 | **88** | - |
| **qwen3-30b** | 84 | **89** | - | - | **89** | **89** | - |
| **qwen3-30b (nt)** | 86 | **92** | - | - | 89 | 87 | - |
| **qwen3-32b** | 90 | **92** | - | - | 80 | 83 | - |
| **qwen3-32b (nt)** | **90** | 88 | - | - | 88 | **90** | - |

- **(t)**: 一貫した改善は確認できず。実用性は限定的。
- 履歴最適値はモデル依存。高性能モデル（gpt-oss-120b、gemma3-27b）は5〜10で高得点に到達、中型モデル（ministral-3-8b、aya-expanse-32b）も10で安定。
- 軽量モデルは履歴増で底上げ傾向（qwen3-4b、gemma3-4bなど）だが、評価の分散も増加。
- 履歴増は遅延・コスト増を伴うため、効果が薄い場合は戻す。

**結論**:
- 既定は --history 10（大多数のモデルで最適またはそれに近い性能）
- 必要時のみ 15/20 を短期検証。分散増・劣化があれば 10 に戻す
- 20超は原則非推奨
- --translated-context は原則非推奨（効果が不安定）

## コンテキスト履歴（--history）オプション詳細

### 概要
`--history N`オプションは過去N件の翻訳履歴をコンテキストとして提供し、対話の一貫性と翻訳品質を向上させます。

### 設定値と効果
- **デフォルト値**: `--history 5`（省略時）
- **推奨設定**: `--history 10`（中・低性能モデルで大幅改善）
- **履歴形式**: 原文と翻訳文の対訳形式でコンテキストに含める

### 実装詳細
```python
# 直前の5つ（またはN個）の翻訳結果をコンテキストとして追加
context_lines = []
if context_history:
    context_lines.append("Previous conversation context:")
    context_lines.append("")
    for ctx in context_history[-N:]:  # Nは--historyで指定
        context_lines.append(f"Original: {ctx['speaker']}: {ctx['original']}")
        context_lines.append(f"Translation: {ctx['speaker']}: {ctx['translation']}")
        context_lines.append("")
```

### モデル別効果
- **高性能モデル** (Gemma3 12B, Qwen3 14B): 改善効果限定的
  - 既に高い文脈理解力を持つため追加履歴の恩恵は小さい
- **中性能モデル** (Phi4 14B): **劇的改善（+12点）**
  - **優れたコンテキスト把握力**により過去の翻訳履歴から一貫性を学習
  - **翻訳の一貫性が大幅向上**し、高性能モデルを凌駕する性能を実現
- **低性能モデル** (Qwen3 4B): 顕著改善（+5点）
  - 履歴情報により翻訳品質が底上げされる

### 関連オプション
- `--translated-context`: 履歴に翻訳文のみを提供（対訳形式でなく）
  - 実験結果では一貫性がなく、通常の対訳形式が推奨

### レベル1: 推論付き翻訳
**特徴**: 5項目詳細推論（構文解析、文脈解釈、語彙選択、文化的配慮、翻訳根拠）
```python
class Translation(BaseModel):
    reasoning: str = Field(description="""Detailed translation reasoning process:
1. Syntactic analysis of the original text...
2. Contextual interpretation of speaker's intent...
3. Evaluation of translation options...
4. Consideration of cultural nuances...
5. Justification for final translation choices...""")
    translation: str = Field(description="Translation result")
```

| モデル | 1-05 | 1-10 | 1-15 | 1-20 | 1-25 |
|:---|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | 77 | **86** | - | 74 | - |
| **aya-expanse-32b** | 82 | **89** | - | 87 | - |
| **command-r7b** | **81** | 64 | - | 79 | - |
| **command-r-35b** | **83** | 81 | - | 67 | - |
| **gemma2-9b** | **84** | 74 | - | 78 | - |
| **gemma3-4b** | 66 | 62 | - | **69** | - |
| **gemma3-12b** | 20 | **23** | - | 21 | - |
| **gemma3-27b** | 89 | **92** | - | 87 | - |
| **gemma3n-e4b** | **80** | 77 | - | 67 | - |
| **gpt-oss-20b** | 87 | **91** | - | 90 | - |
| **gpt-oss-120b** | **92** | **92** | - | 91 | - |
| **llama3.3** | **88** | 83 | - | 86 | - |
| **llama4-scout** | **64** | 25 | - | 49 | - |
| **ministral-3-3b** | **32** | 24 | - | 23 | - |
| **ministral-3-8b** | 27 | 28 | - | **34** | - |
| **ministral-3-14b** | 76 | **85** | - | 81 | - |
| **mistral-small3.2** | **85** | 82 | - | 84 | - |
| **mixtral-8x7b** | 80 | 58 | - | **82** | - |
| **mixtral-8x22b** | 72 | 86 | - | **88** | - |
| **phi4** | 72 | 59 | - | **84** | - |
| **qwen3-4b** | **80** | **80** | 79 | 70 | 77 |
| **qwen3-4b (nt)** | **81** | 76 | 75 | 75 | 73 |
| **qwen3-4b (t)** | 80 | 80 | 79 | **81** | 77 |
| **qwen3-4b (nt,t)** | 76 | 74 | **81** | 74 | 73 |
| **qwen3-14b** | 89 | **92** | - | 89 | - |
| **qwen3-14b (nt)** | 85 | 82 | - | **87** | - |
| **qwen3-30b** | 86 | **88** | - | **88** | - |
| **qwen3-30b (nt)** | 84 | **92** | - | 89 | - |
| **qwen3-32b** | 85 | **90** | - | 89 | - |
| **qwen3-32b (nt)** | **74** | 51 | - | 66 | - |

- **(t)**: 出力形式は安定化するが、得点向上は不安定。
- 履歴による改善は限定的で、レベル0を超えるケースは少数。
- **高得点を維持できるモデル**: gpt-oss-120b（92点、履歴5/10）、gpt-oss-20b（91点、履歴10）、qwen3-14b（92点、履歴10）、qwen3-30b（88点、履歴10/20）、qwen3-32b（90点、履歴10）、gemma3-27b（92点、履歴10）。
- **大幅劣化が顕著**: gemma3-12b（20点台、出力崩壊）、ministral-3-3b/8b（20点台）、llama4-scout（履歴10で25点）は構造化推論に不適合。
- **中型以上は安定傾向**: aya-expanse-32b、mistral-small3.2、mixtral-8x22b は履歴10〜20で80点台後半を維持。

**結論**:
- 実運用は原則レベル0。レベル1は教育・思考可視化に限定。
- 使用する場合は、gpt-oss系、qwen3系（14b以上）、gemma3-27bなど高性能モデルで履歴10を推奨。

### レベル2: 2段階翻訳
**特徴**: 直接翻訳後、推敲して翻訳文を生成する2段階翻訳
```python
class Translation(BaseModel):
    draft_translation: str = Field(description="First draft translation")
    quality_assessment: str = Field(description="Analyze translation for errors, mistranslations, language mixing...")
    improvement_suggestions: str = Field(description="Provide specific suggestions for improving quality")
    improved_translation: str = Field(description="Improved translation based on assessment")
```

| モデル | 2-05 | 2-10 | 2-15 | 2-20 | 2-25 |
|:---|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | **92** | 89 | - | 72 | - |
| **aya-expanse-32b** | 79 | 78 | - | **84** | - |
| **command-r7b** | 72 | 73 | - | **76** | - |
| **command-r-35b** | 84 | 83 | - | **88** | - |
| **gemma2-9b** | 87 | 81 | - | **91** | - |
| **gemma3-4b** | **75** | 71 | - | 74 | - |
| **gemma3-12b** | 86 | 85 | - | **89** | - |
| **gemma3-27b** | 83 | 80 | - | **87** | - |
| **gemma3n-e4b** | **89** | 85 | - | 80 | - |
| **gpt-oss-20b** | 89 | **91** | - | **91** | - |
| **gpt-oss-120b** | 91 | 90 | - | **92** | - |
| **llama3.3** | 72 | 81 | - | **86** | - |
| **llama4-scout** | 74 | **75** | - | 68 | - |
| **ministral-3-3b** | 68 | **69** | - | 62 | - |
| **ministral-3-8b** | 84 | **92** | - | 86 | - |
| **ministral-3-14b** | 50 | **78** | - | 74 | - |
| **mistral-small3.2** | 90 | **91** | - | 84 | - |
| **mixtral-8x7b** | 62 | **77** | - | 75 | - |
| **mixtral-8x22b** | **82** | 79 | - | 80 | - |
| **phi4** | 82 | **92** | - | 90 | - |
| **qwen3-4b** | 76 | **85** | 82 | 77 | 80 |
| **qwen3-4b (nt)** | 80 | **84** | 82 | 81 | 80 |
| **qwen3-4b (t)** | 81 | 83 | 81 | **84** | 74 |
| **qwen3-4b (nt,t)** | 73 | 69 | 75 | **86** | 75 |
| **qwen3-14b** | 80 | **90** | - | 77 | - |
| **qwen3-14b (nt)** | **87** | **87** | - | 86 | - |
| **qwen3-30b** | 89 | 85 | - | **92** | - |
| **qwen3-30b (nt)** | 88 | **90** | - | 87 | - |
| **qwen3-32b** | 87 | 86 | - | **91** | - |
| **qwen3-32b (nt)** | 87 | **88** | - | 87 | - |

- 中〜小型の底上げに有効。特にphi4（履歴10で92点）、ministral-3-8b（履歴10で92点）、qwen3-14b（履歴10で90点）が顕著な改善を示す。
- 高性能モデルはレベル0と同等: gpt-oss-120b（履歴20で92点）、gpt-oss-20b（履歴10/20で91点）、mistral-small3.2（履歴10で91点）。
- aya-expanse-8bは履歴5で92点と大幅改善、gemma3n-e4bも履歴5で89点と底上げされる。
- 履歴増で劣化するモデルも存在: aya-expanse-8b、gemma3n-e4bは履歴20で大幅低下。
- ministral-3は8bのみ高得点、3b/14bは低迷（特に14bは履歴5で50点）。
- **(t)** は qwen3-4b で効果が不安定（履歴20で若干改善）。

**結論**: 中小型モデルで品質向上効果が顕著。レベル0と同等以上ならレベル2を採用、そうでなければ速度面で0を優先。

### 翻訳改善効果の検証（レベル0 vs レベル2）

レベル0（直接翻訳）とレベル2（2段階翻訳）の性能比較により、2段階翻訳による改善効果を検証

| モデル | 0-05 | 0-10 | 0-20-a | 0-20-b | 2-05 | 2-10 | 2-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | 83 | 87 | 85 | 80 | **92** | 89 | 72 |
| **aya-expanse-32b** | 85 | **92** | 91 | **92** | 79 | 78 | 84 |
| **command-r7b** | 86 | 75 | **87** | 73 | 72 | 73 | 76 |
| **command-r-35b** | **91** | 90 | 81 | 90 | 84 | 83 | 88 |
| **gemma2-9b** | 78 | 76 | 78 | 87 | 87 | 81 | **91** |
| **gemma3-4b** | 75 | **87** | 86 | 83 | 75 | 71 | 74 |
| **gemma3-12b** | **91** | 90 | 88 | 90 | 86 | 85 | 89 |
| **gemma3-27b** | **92** | **92** | 91 | **92** | 83 | 80 | 87 |
| **gemma3n-e4b** | 86 | **91** | **91** | 89 | 89 | 85 | 80 |
| **gpt-oss-20b** | 86 | 81 | 89 | 89 | 89 | **91** | **91** |
| **gpt-oss-120b** | 91 | 89 | 91 | 88 | 91 | 90 | **92** |
| **llama3.3** | 85 | 82 | 82 | 85 | 72 | 81 | **86** |
| **llama4-scout** | 85 | 88 | 91 | **92** | 74 | 75 | 68 |
| **ministral-3-3b** | 83 | 77 | **88** | 81 | 68 | 69 | 62 |
| **ministral-3-8b** | 91 | **92** | 90 | 91 | 84 | **92** | 86 |
| **ministral-3-14b** | **90** | 85 | 86 | 86 | 50 | 78 | 74 |
| **mistral-small3.2** | **91** | 88 | 89 | **91** | 90 | **91** | 84 |
| **mixtral-8x7b** | 81 | **88** | 81 | 80 | 62 | 77 | 75 |
| **mixtral-8x22b** | 85 | 85 | 85 | **92** | 82 | 79 | 80 |
| **phi4** | 85 | 90 | 89 | 88 | 82 | **92** | 90 |
| **qwen3-4b** | 75 | **88** | 84 | 81 | 76 | 85 | 77 |
| **qwen3-4b (nt)** | 81 | 77 | - | - | 80 | **84** | 81 |
| **qwen3-14b** | 87 | 78 | 83 | 88 | 80 | **90** | 77 |
| **qwen3-14b (nt)** | 85 | 86 | - | - | **87** | **87** | 86 |
| **qwen3-30b** | 84 | 89 | 89 | 89 | 89 | 85 | **92** |
| **qwen3-30b (nt)** | 86 | **92** | - | - | 88 | 90 | 87 |
| **qwen3-32b** | 90 | **92** | 80 | 83 | 87 | 86 | 91 |
| **qwen3-32b (nt)** | **90** | 88 | - | - | 87 | 88 | 87 |

- **レベル2で明確に改善**: phi4（0-10: 90点→2-10: 92点）、gpt-oss-20b（0-10: 81点→2-10: 91点、+10点）、aya-expanse-8b（0-10: 87点→2-05: 92点、+5点）。
- **高得点モデルは同等**: gpt-oss-120b、gemma3-27b、gemma3-12bはレベル0/2とも90点前後で差は僅少。
- **レベル0が優位**: llama4-scout（0-20-b: 92点→2-05: 74点、-18点）、ministral-3-3b（0-20-a: 88点→2-05: 68点、-20点）、ministral-3-14b（0-05: 90点→2-10: 78点、-12点）など、一部モデルでレベル2が大幅劣化。
- **履歴依存の最適化**: gemma2-9bはレベル2-20で91点と高得点だが、レベル0では87点が最高。qwen3-30bも2-20で92点とピーク。
- gemma3-4bは2段階化で劣化（0-10: 87点→2-10: 71点）。
- gemma3n-e4bは履歴5で両レベルとも高得点、履歴増で劣化。速度重視なら0-10が実用的。

**結論**: 改善効果が明確なモデル（phi4、gpt-oss-20b、aya-expanse-8b）ではレベル2を採用。差が僅少または劣化するならレベル0を優先。

### レベル3: 推論付き2段階翻訳

**特徴**: レベル1の推論とレベル2の2段階翻訳を統合
```python
class Translation(BaseModel):
    reasoning: str = Field(description="Detailed translation reasoning process...")
    draft_translation: str = Field(description="First draft translation")
    quality_assessment: str = Field(description="Analyze the draft translation for errors...")
    improvement_suggestions: str = Field(description="Provide specific suggestions...")
    improved_translation: str = Field(description="Based on the quality assessment...")
```
- **処理**: 推論→下訳→品質評価→改善提案→最終翻訳
- **利点**: 最も詳細な処理、全プロセス可視化
- **用途**: 研究目的、品質分析
- **実験結果**: レベル2に比べて明確な改善が認められない

### レベル4: 推論付き2段階翻訳の分割

**特徴**: レベル3を2つのステージに分割実行
```python
# 第1段階
class FirstStageTranslation(BaseModel):
    reasoning: str = Field(description="Detailed translation reasoning process...")
    draft_translation: str = Field(description="First draft translation")

# 第2段階
class SecondStageTranslation(BaseModel):
    quality_assessment: str = Field(description="Analyze the draft translation for errors...")
    improvement_suggestions: str = Field(description="Provide specific suggestions...")
    improved_translation: str = Field(description="Based on the quality assessment...")
```
- **処理**: ステージ1（推論+下訳）→ステージ2（品質評価+改善）
- **利点**: 段階的制御、メモリ効率化
- **用途**: 大規模翻訳、実験的処理
- **実験結果**: レベル3に比べて劣化傾向（コンテキスト分断の悪影響）

## 複雑な推論の逆効果メカニズム

客観評価により判明した、複雑推論による品質悪化の原因：

### 1. 翻訳選択肢の増加による混乱
- 推論プロセスで複数の翻訳候補を検討
- 選択肢が増えることで決定が不安定化
- 結果として一貫性のない翻訳が生成

### 2. 一貫性よりも局所最適化の優先
- 各段階で最適化を図るが全体最適を見失う
- 部分的な改善が全体品質を損なう
- フェーズ間での情報ロスが発生

### 3. 複雑な思考プロセスによる判断の不安定化
- 推論が深くなるほど迷いが生じる
- 自己評価による混乱が品質低下を招く
- シンプルな直接翻訳の方が安定した結果

## translate4.py: 非構造化直接翻訳による構造化制約の検証

構造化出力（translate.py -r 0）と非構造化出力（translate4.py）の直接翻訳性能を比較し、構造化制約の影響を検証しました。

### 核心的な発見

**構造化出力vs非構造化出力の比較実験**により、モデル特性に応じた個別最適化の重要性が判明：

| 推論方式 | 構造化出力 | 非構造化出力 | 性能差（例：Gemma3 12B, h05） |
|:---|:---|:---|:---|
| **直接翻訳** | レベル0: 89点 | translate4: 79点 | **-10点** |

### 重要な結論

1. **構造化出力の効果はモデル依存**: 全般的悪影響は存在せず、モデル・履歴数の組み合わせに強く依存
2. **個別最適化の重要性**: 画一的判断を避け、モデル特性に応じた設定が必要
3. **Reasoning制御の価値**: reasoning処理の制御が構造化出力制約よりも性能に大きく影響
4. **実行時安定性の考慮**: 評価スコアと実際の安定性が一致しない場合があり、実用性重視の選択が重要

**最適化戦略**: 
- **構造化出力優位**: Gemma3 12B、Gemma3 4B、Qwen3 14B（3/7モデル、43%）
- **非構造化出力優位**: Gemma2 9B、Gemma3n E4B、Phi4（3/7モデル、43%）
- **高得点での均衡**: 90点以上では構造化・非構造化が拮抗（各50%）

### 直接翻訳における構造化出力の影響調査（レベル0 vs tr4）

[translate.py](translate.py) の `-r 0` オプションで構造化出力、[translate4.py](translate4.py) で非構造化出力

| モデル | 0-05 | 0-10 | 0-20-a | 0-20-b | tr4-05 | tr4-10 | tr4-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | 83 | **87** | 85 | 80 | 84 | 85 | 80 |
| **aya-expanse-32b** | 85 | **92** | 91 | **92** | 88 | 87 | 90 |
| **command-r7b** | 86 | 75 | **87** | 73 | 79 | 77 | 78 |
| **command-r-35b** | 91 | 90 | 81 | 90 | **92** | 90 | 91 |
| **gemma2-9b** | 78 | 76 | 78 | **87** | 79 | 80 | 76 |
| **gemma3-4b** | 75 | **87** | 86 | 83 | 75 | 71 | 85 |
| **gemma3-12b** | **91** | 90 | 88 | 90 | 85 | 90 | **91** |
| **gemma3-27b** | **92** | **92** | 91 | **92** | **92** | **92** | **92** |
| **gemma3n-e4b** | 86 | **91** | **91** | 89 | 82 | 86 | 84 |
| **gpt-oss-20b** | 86 | 81 | 89 | 89 | 85 | **91** | 84 |
| **gpt-oss-120b** | 91 | 89 | 91 | 88 | **92** | 88 | 90 |
| **llama3.3** | 85 | 82 | 82 | 85 | **91** | 86 | 88 |
| **llama4-scout** | 85 | 88 | 91 | **92** | **92** | 91 | 80 |
| **ministral-3-3b** | 83 | 77 | **88** | 81 | 79 | 76 | 69 |
| **ministral-3-8b** | 91 | **92** | 90 | 91 | 89 | 85 | 83 |
| **ministral-3-14b** | **90** | 85 | 86 | 86 | 82 | 85 | 79 |
| **mistral-small3.2** | **91** | 88 | 89 | **91** | 88 | **91** | 85 |
| **mixtral-8x7b** | 81 | **88** | 81 | 80 | 71 | 67 | 72 |
| **mixtral-8x22b** | 85 | 85 | 85 | **92** | 87 | 76 | 77 |
| **phi4** | 85 | 90 | 89 | 88 | 83 | 87 | **91** |
| **qwen3-4b** | 75 | **88** | 84 | 81 | 62 | 64 | 81 |
| **qwen3-4b (nt)** | 81 | 77 | - | - | 79 | 74 | **83** |
| **qwen3-14b** | 87 | 78 | 83 | 88 | **89** | 85 | 77 |
| **qwen3-14b (nt)** | 85 | 86 | - | - | **91** | 85 | 85 |
| **qwen3-30b** | 84 | **89** | **89** | **89** | 58 | 48 | 50 |
| **qwen3-30b (nt)** | 86 | **92** | - | - | 0 | 0 | 0 |
| **qwen3-32b** | 90 | **92** | 80 | 83 | 80 | **92** | 83 |
| **qwen3-32b (nt)** | **90** | 88 | - | - | 83 | 75 | 82 |

- **構造化出力優位**: aya-expanse-32b（0-10: 92点→tr4-10: 87点、-5点）、gemma3n-e4b（0-10: 91点→tr4-10: 86点、-5点）、ministral-3-8b（0-10: 92点→tr4-10: 85点、-7点）、qwen3-30b（0-10: 89点→tr4-05: 58点、-31点、tr4で暴走）。
- **非構造化出力優位**: llama3.3（tr4-05: 91点→0-05: 85点、+6点）、llama4-scout（tr4-05: 92点→0-05: 85点、+7点）、command-r-35b（tr4-05: 92点→0-05: 91点、+1点）、qwen3-14b（tr4-nt-05: 91点→0-nt-05: 85点、+6点）。
- **同等水準**: gemma3-27b（両方式で92点）、gpt-oss-120b（両方式で91〜92点）、mistral-small3.2（両方式で91点）、phi4（tr4-20: 91点、0-10: 90点、ほぼ同等）。
- **重要な発見**: qwen3-30b (nt) はtr4で完全に失敗（0点）。構造化出力が必須。
- **(nt)**: tr4では明確に改善するケースがある（qwen3-4b (nt): tr4-20で83点、qwen3-14b (nt): tr4-05で91点）。Ollama制約でreasoning無効化時はレベル0とほぼ同等。
- gemma3-4bは構造化が優位（0-10: 87点→tr4-10: 71点、-16点）。
- mixtral-8x7bは両方式で低迷だが、構造化がやや優位（0-10: 88点→tr4-10: 67点、-21点）。

**結論**: 同等性能ならパース安定性から構造化（レベル0）を優先。qwen3-30bは構造化必須。llama系、command-r-35bはtr4が優位。

## translate5.py: 自由記述式推論による構造化出力制約の検証

レベル1での壊滅的失敗（gemma3-12b: 11点）に対処するため、構造化出力の制約を除去した自由記述式推論実験を実施しました。

※ gemma3-12b 以外のモデルでの結果は tr6 との比較を参照

### 推論プロンプト

```
First, briefly analyze the text for:
1. Key vocabulary and expressions
2. Speaker's intent and tone
3. Cultural context and appropriate register

Then provide your final translation on the last line.
```

### 実験結果 (gemma3-12b)

**構造化 vs 非構造化推論の対比実験**により、同じ推論プロセスでも実装方法で劇的に性能が変わることを実証：

| 推論方式 | 構造化出力 | 非構造化出力 | 性能差 |
|:---|:---|:---|:---|
| **直接翻訳** | レベル0: 95点 | translate4: 79点 | **-16点** |
| **推論付き翻訳** | レベル1: 11点 | translate5: 93点 | **+82点** |

1. **構造化出力制約の有害性実証**: レベル1（11点）→ translate5（93点）の+82点改善
2. **推論プロセス自体の限界**: レベル0（95点）に対してtranslate5（93点）は-2点の軽微な劣化
3. **言語化によるオーバーヘッド**: 明示的分析が注意容量を圧迫し翻訳品質を阻害
4. **後付け説明方式の優位性**: 品質とのトレードオフを回避する設計原則を提示

**結論**: 翻訳タスクにおいては直接翻訳（レベル0）が最適解。推論プロセスは構造化制約下では有害、自由記述式では品質をほぼ維持するが、コストパフォーマンスで直接翻訳に劣る。

## translate6.py: 改良された自由記述式推論による比較実験

translate5.pyの推論プロンプトを改良し、レベル1との公平な比較を実現するバージョンです。

### 改造の目的

translate5.pyの初期実験では、推論内容がレベル1と異なっていたため、公平な比較ができませんでした。translate6.pyは以下の改良を実施：

1. **推論内容の統一**: translate.py -r 1と同じ5項目の詳細推論を実装
2. **翻訳選択肢の検討**: 重要語彙や慣用表現の翻訳オプション評価を追加
3. **根拠の明確化**: 最終的な翻訳選択の正当化プロセスを強化

### 改良された推論プロンプト

```
First, provide detailed translation reasoning covering:
1. Syntactic analysis of the original {args.from_lang} text (subject, predicate, object, modifiers, etc.)
2. Contextual interpretation of speaker's intent and emotional tone
3. Evaluation of {args.to_lang} translation options for key vocabulary and idiomatic expressions
4. Consideration of cultural nuances and appropriate register/politeness level
5. Justification for final {args.to_lang} translation choices and overall approach

Then provide your final translation on the last line.
```

### 実験の価値

translate6.pyにより、構造化出力制約の影響をより正確に測定し、レベル1との真の性能差を検証可能になります。特に「どのように翻訳するか」という視点を含む完全な推論プロセスの比較が実現されます。

### 推論付き翻訳における構造化出力の影響調査（レベル1 vs tr6）

[translate.py](translate.py) の `-r 1` オプションで構造化出力、[translate6.py](translate6.py) で非構造化出力

| モデル | 1-05 | 1-10 | 1-20 | tr6-05 | tr6-10 | tr6-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | 77 | **86** | 74 | 80 | 75 | 80 |
| **aya-expanse-32b** | 82 | 89 | 87 | 88 | **92** | 83 |
| **command-r7b** | **81** | 64 | 79 | 75 | 75 | 65 |
| **command-r-35b** | **83** | 81 | 67 | 73 | 78 | 81 |
| **gemma2-9b** | **84** | 74 | 78 | 80 | 77 | **84** |
| **gemma3-4b** | 66 | 62 | 69 | 82 | 84 | **85** |
| **gemma3-12b** | 20 | 23 | 21 | 85 | **90** | 85 |
| **gemma3-27b** | 89 | **92** | 87 | 86 | **92** | 90 |
| **gemma3n-e4b** | **80** | 77 | 67 | **80** | **80** | 78 |
| **gpt-oss-20b** | 87 | 91 | 90 | **92** | **92** | 91 |
| **gpt-oss-120b** | **92** | **92** | 91 | 88 | 91 | 89 |
| **llama3.3** | **88** | 83 | 86 | 85 | 84 | **88** |
| **llama4-scout** | 64 | 25 | 49 | 85 | **89** | 81 |
| **ministral-3-3b** | 32 | 24 | 23 | 72 | **87** | 85 |
| **ministral-3-8b** | 27 | 28 | 34 | 84 | 85 | **89** |
| **ministral-3-14b** | 76 | 85 | 81 | 76 | 89 | **91** |
| **mistral-small3.2** | 85 | 82 | 84 | **91** | 40 | 88 |
| **mixtral-8x7b** | 80 | 58 | **82** | 31 | 32 | 38 |
| **mixtral-8x22b** | 72 | 86 | **88** | 87 | 84 | 84 |
| **phi4** | 72 | 59 | 84 | **89** | 86 | 82 |
| **qwen3-4b** | 80 | 80 | 70 | **82** | 78 | 71 |
| **qwen3-4b (nt)** | **81** | 76 | 75 | 74 | 77 | 73 |
| **qwen3-14b** | 89 | **92** | 89 | 81 | 90 | 83 |
| **qwen3-14b (nt)** | 85 | 82 | 87 | 85 | **88** | 84 |
| **qwen3-30b** | 86 | **88** | **88** | 11 | 12 | 14 |
| **qwen3-30b (nt)** | 84 | **92** | 89 | 0 | 0 | 0 |
| **qwen3-32b** | 85 | 90 | 89 | 90 | **91** | 87 |
| **qwen3-32b (nt)** | 74 | 51 | 66 | **90** | 88 | 83 |

- **tr6で劇的改善**: gemma3-12b（1-10: 23点→tr6-10: 90点、+67点）、phi4（1-10: 59点→tr6-05: 89点、+30点）、gemma3-4b（1-10: 62点→tr6-20: 85点、+23点）、llama4-scout（1-10: 25点→tr6-10: 89点、+64点）、ministral-3-3b（1-10: 24点→tr6-10: 87点、+63点）、ministral-3-8b（1-10: 28点→tr6-20: 89点、+61点）。
- **高性能モデルも改善**: gpt-oss-20b（1-10: 91点→tr6-05: 92点、ただし1系でも十分高得点）、mistral-small3.2（1-05: 85点→tr6-05: 91点、+6点）、qwen3-32b (nt)（1-nt-10: 51点→tr6-nt-05: 90点、+39点）。
- **構造化が優位**: qwen3-14b（1-10: 92点→tr6-10: 90点、-2点）、qwen3-30b（1-10: 88点→tr6-10: 12点、-76点、tr6で暴走）、gpt-oss-120b（1-05: 92点→tr6-05: 88点、-4点）。
- **重要な発見**: qwen3-30b (nt) はtr6で完全に失敗（0点）。構造化（1系）が必須。
- mixtral-8x7bは両方式で低迷（tr6-05: 31点、1-05: 80点、構造化が相対的にマシ）。

**結論**: 構造化推論で大幅劣化するモデル（gemma3-12b、ministral-3系、llama4-scout、phi4など）ではtr6を第一選択。高性能モデル（gpt-oss系、qwen3-14b）は構造化でも高得点。qwen3-30bは構造化が必須。

### 自由記述式推論比較（tr5 vs tr6）

[translate5.py](translate5.py) は簡略推論、[translate6.py](translate6.py) は詳細推論

| モデル | tr5-05 | tr5-10 | tr5-20 | tr6-05 | tr6-10 | tr6-20 |
|:---|:---:|:---:|:---:|:---:|:---:|:---:|
| **aya-expanse-8b** | 76 | 77 | 67 | **80** | 75 | **80** |
| **aya-expanse-32b** | 86 | 87 | 84 | 88 | **92** | 83 |
| **command-r7b** | 72 | **78** | 35 | 75 | 75 | 65 |
| **command-r-35b** | 85 | **88** | 83 | 73 | 78 | 81 |
| **gemma2-9b** | 81 | 76 | **86** | 80 | 77 | 84 |
| **gemma3-4b** | **88** | 87 | **88** | 82 | 84 | 85 |
| **gemma3-12b** | 87 | 90 | **92** | 85 | 90 | 85 |
| **gemma3-27b** | 89 | **92** | 88 | 86 | **92** | 90 |
| **gemma3n-e4b** | **85** | 84 | 75 | 80 | 80 | 78 |
| **gpt-oss-20b** | 91 | 88 | 85 | **92** | **92** | 91 |
| **gpt-oss-120b** | **91** | 88 | **91** | 88 | **91** | 89 |
| **llama3.3** | 88 | 87 | **89** | 85 | 84 | 88 |
| **llama4-scout** | 82 | 72 | 78 | 85 | **89** | 81 |
| **ministral-3-3b** | 65 | 80 | 78 | 72 | **87** | 85 |
| **ministral-3-8b** | 83 | 72 | 78 | 84 | 85 | **89** |
| **ministral-3-14b** | 72 | 85 | 90 | 76 | 89 | **91** |
| **mistral-small3.2** | 86 | 90 | 88 | **91** | 40 | 88 |
| **mixtral-8x7b** | 20 | 12 | 32 | 31 | 32 | **38** |
| **mixtral-8x22b** | 69 | 81 | 76 | **87** | 84 | 84 |
| **phi4** | 85 | **89** | 84 | **89** | 86 | 82 |
| **qwen3-4b** | 72 | 70 | **86** | 82 | 78 | 71 |
| **qwen3-4b (nt)** | 25 | 47 | 50 | 74 | **77** | 73 |
| **qwen3-14b** | 89 | 84 | 86 | 81 | **90** | 83 |
| **qwen3-14b (nt)** | 85 | 83 | **88** | 85 | **88** | 84 |
| **qwen3-30b** | 22 | 19 | **26** | 11 | 12 | 14 |
| **qwen3-30b (nt)** | **0** | **0** | **0** | **0** | **0** | **0** |
| **qwen3-32b** | 83 | 85 | 85 | 90 | **91** | 87 |
| **qwen3-32b (nt)** | **90** | 89 | **90** | **90** | 88 | 83 |

- **tr6で改善**: aya-expanse-32b（tr5-10: 87点→tr6-10: 92点、+5点）、gpt-oss-20b（tr5-05: 91点→tr6-05/10: 92点、+1点）、llama4-scout（tr5-05: 82点→tr6-10: 89点、+7点）、ministral-3-3b（tr5-10: 80点→tr6-10: 87点、+7点）、ministral-3-8b（tr5-05: 83点→tr6-20: 89点、+6点）、ministral-3-14b（tr5-20: 90点→tr6-20: 91点、+1点）。
- **tr5で優位**: gemma3-4b（tr5-05/20: 88点→tr6-20: 85点、-3点）、gemma3-12b（tr5-20: 92点→tr6-10: 90点、-2点）、gemma3n-e4b（tr5-05: 85点→tr6-05/10: 80点、-5点）、llama3.3（tr5-20: 89点→tr6-20: 88点、-1点）、qwen3-4b（tr5-20: 86点→tr6-05: 82点、-4点）。
- **同等水準**: gemma3-27b（両方式で92点）、gpt-oss-120b（両方式で91点）、phi4（両方式で89点）。
- **両方式で失敗**: qwen3-30b（tr5/tr6ともに10点台、ntで0点）、mixtral-8x7b（tr5-05: 20点、tr6-20: 38点、両方式で低迷）。
- 軽量モデル（gemma3-4b、gemma2-9b）はtr5で相対的に善戦（86〜88点）。

**結論**: tr6は中型以上で改善効果あり（特にministral-3系、llama4-scout、gpt-oss-20b）。軽量モデルはtr5が優位。実用はモデル特性に応じた選択を推奨。qwen3-30bは両方式で不適合。

## モデル別実用設定一覧

各モデルの上位3項目（90点以上）または最高点1項目をリストアップ。

| モデル | スコア | 設定 |
|:---|:---:|:---|
| **aya-expanse-32b** | 92 | 0-10, 0-20-b, tr6-10 |
| **aya-expanse-32b** | 91 | 0-20-a |
| **aya-expanse-8b** | 92 | 2-05 |
| **command-r-35b** | 92 | tr4-05 |
| **command-r-35b** | 91 | 0-05, tr4-20 |
| **gemma3-12b** | 92 | tr5-20 |
| **gemma3-12b** | 91 | 0-05, tr4-20 |
| **gemma3-27b** | 92 | 0, 0-05, 0-10, 0-20-b, 1-10, tr4-05, tr4-10, tr4-20, tr5-10, tr6-10 |
| **gemma3-27b** | 91 | 0-20-a |
| **gpt-oss-120b** | 92 | 0, 1-05, 1-10, 2, 2-20, 3, tr4-05 |
| **gpt-oss-120b** | 91 | 0-05, 0-20-a, 1-20, 2-05, tr5-05, tr5-20, tr6-10 |
| **gpt-oss-20b** | 92 | tr6-05, tr6-10 |
| **gpt-oss-20b** | 91 | 1-10, 2-10, 2-20, 4, tr4-10, tr5-05, tr6-20 |
| **llama4-scout** | 92 | 0-20-b, tr4-05 |
| **llama4-scout** | 91 | 0-20-a, tr4-10 |
| **ministral-3-8b** | 92 | 0-10, 2-10 |
| **ministral-3-8b** | 91 | 0-05, 0-20-b |
| **mixtral-8x22b** | 92 | 0-20-b |
| **phi4** | 92 | 2-10 |
| **phi4** | 91 | tr4-20 |
| **qwen3-14b** | 92 | 1-10 |
| **qwen3-14b** | 91 | 0, 2 |
| **qwen3-14b (nt)** | 91 | tr4-nt-05 |
| **qwen3-30b (nt)** | 92 | 0-nt-10, 1-nt, 1-nt-10 |
| **qwen3-30b** | 92 | 2-20 |
| **qwen3-32b (nt)** | 92 | 2-nt |
| **qwen3-32b** | 92 | 0-10, tr4-10 |
| **qwen3-32b (nt)** | 91 | 1-nt |
| **gemma2-9b** | 91 | 2-20 |
| **gemma3n-e4b** | 91 | 0-10, 0-20-a |
| **llama3.3** | 91 | tr4-05 |
| **ministral-3-14b** | 91 | tr6-20 |
| **mistral-small3.2** | 91 | 0-05, 0-20-b, 2-10, 3, tr4-10, tr6-05 |
| **gemma3-4b** | 88 | tr5-05, tr5-20 |
| **ministral-3-3b** | 88 | 0-20-a |
| **mixtral-8x7b** | 88 | 0-10 |
| **qwen3-4b** | 88 | 0-10 |
| **command-r7b** | 87 | 0-20-a |

※ 85 点未満は実用には不向きです。

## 推奨モデルと設定

**注意**: Gemma と Llama は生成物にライセンス上の制約があります。Cohere 社のモデル (Command R, Aya Expanse) は商用利用不可です。

### CPU 実行環境

1. **gemma3n-e4b** (91点):
   - **0-10, 0-20-a**: 速度優先、構造化出力で安定

2. **gemma2-9b** (91点):
   - **2-20**: 教育利用

※ メモリが 32GB 以上ある場合は gpt-oss-20b も選択肢（tr6-05/10で92点、高品質）。

### GPU 実行環境

1. **gemma3-27b** (92点、全設定で安定):
   - **0-05/10, 0-20-b**: 速度優先、構造化直接翻訳
   - **tr4-05/10/20**: 速度優先、非構造化直接翻訳
   - **1-10, tr5-10, tr6-10**: 教育利用（推論プロセス可視化）
   - 構造化・非構造化とも同等の高性能、コンテキスト履歴の影響は限定的

2. **gpt-oss-120b** (92点、巨大モデル):
   - **0, 1-05/10, 2, 2-20, 3, tr4-05**: 速度優先・教育利用

3. **gpt-oss-20b** (92点):
   - **tr6-05/10**: 品質優先・教育利用（全実験で安定した高得点）

4. **qwen3-32b** (92点):
   - **0-10, tr4-10**: 速度優先
   - **2-nt**: 構造化2段階翻訳

5. **ministral-3-8b** (92点、サイズの割に高性能):
   - **0-10, 2-10**: 速度優先・2段階翻訳
   - **注意**: レベル1では大幅劣化（27-34点）

6. **phi4** (92点):
    - **2-10**: 2段階翻訳で最高得点（0-10から+2点改善）

7. **ministral-3-14b** (91点):
    - **tr6-20**: 教育利用、非構造化推論で大幅改善

**教育利用**: レベル 2 と tr6 の 5 項目推論（構文解析、文脈解釈、語彙選択、文化的配慮、翻訳根拠）は人間の翻訳思考プロセスに近く、語学学習に有用。

## まとめ

### 構造化出力の特性
1. **最低性能要件**: gemma3-4bでは出力が頻繁に破綻
2. **複雑指示の限界**: レベル1は多くのモデルで処理能力を超過
3. **Ollamaの制約活用**: 構造化出力時のreasoning自動無効化が有効に作用

### 最適化戦略
1. **高性能モデル**: レベル0（直接翻訳）で最高品質
2. **中性能モデル**: レベル2 + 適切な履歴設定で大幅改善
3. **低性能モデル**: 非構造化出力の活用を検討

### オプション効果の総括
**--translated-context (t)**:
- 効果が限定的かつ不安定、非推奨

**--no-think (nt)**:
- qwen3系でむしろ性能向上、大幅な速度改善
- Reasoningモデルでの翻訳タスクには有効

## 実用システム構築の指針

### モデル選択の優先順位
1. **gpt-ossシリーズ**: 安定した高品質
2. **qwenシリーズ**: 活発な開発、安定した品質
3. **gemmaシリーズ**: ライセンス制約を考慮して選択

### パフォーマンス vs 品質のバランス
- **明確な差異がない場合**: 処理速度優先でレベル0を選択
- **品質重視**: モデル特性に応じた個別最適化
- **教育用途**: 推論プロセス可視化のためtr6を活用

### 実装時の注意点
1. **スコア変動の考慮**: 単一評価でなく複数回測定を推奨
2. **安定性重視**: 評価スコアと実際の運用安定性は必ずしも一致しない
3. **リソース効率**: 複雑なシステムが常に高品質とは限らない

## 結論

本実験により、「複雑な推論システム = 高品質翻訳」という従来仮説は客観評価により否定され、「適切なモデル選択 + シンプルな直接出力 = 最高効率」が実証されました。

構造化出力による推論制御は教育的価値は高いものの、実用翻訳においては：
- **高性能モデル**: シンプルな直接翻訳が最適
- **中・低性能モデル**: 適切なコンテキスト履歴活用により高性能モデルを凌駕可能

これらの知見は翻訳以外の言語タスク（文章要約、コード生成、創作支援）にも応用可能ですが、常に**シンプルな直接出力の有効性**を検証することが重要です。

## その他の実験ツール一覧

以下のPythonスクリプトは研究開発過程で作成された実験的ツールです。詳細な実装内容と評価結果については [OBSOLETE.md](OBSOLETE.md) を参照してください。

- **translate-exp.py**: サブコマンド方式の多段階翻訳システム（Phase 1/2/2a/3対応）
- **translate2.py**: 3段階多モデル翻訳 一気通貫版
- **translate3.py**: Phase 2a統合システム 一気通貫版（推奨）
- **draft_to_text.py**: JSON形式中間データからテキスト抽出ユーティリティ
- **analyze_2stage_diff.py**: 翻訳手法別品質差分析ツール

これらのツールは実験的性質が強く、客観的評価によりレベル0（直接翻訳）の優位性が判明したため、研究記録として保持されています。
