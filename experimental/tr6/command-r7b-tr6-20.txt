Camille: Hola y bienvenidos a «Tech Éclair», el podcast donde desciframos la tecnología que da forma a nuestro mundo. Soy Camille.
Luc: Y soy Luc. Hoy vamos a levantar el telón sobre cómo aprenden los modelos de IA que utilizamos en nuestro día a día y llegan a ser tan inteligentes.
Camille: "Es un tema fascinante. A menudo percibimos estas IA como cajas negras, pero su aprendizaje sigue un proceso bien real."
Luc: Exactamente. Y este aprendizaje comienza con un proceso llamado « pre-entrenamiento ».
Camille: El pre-entrenamiento.
Luc: Imaginemos que enviamos a una IA completamente nueva a la escuela para darle cultura general. Lee una gran cantidad de datos de Internet para aprender los fundamentos del lenguaje, el razonamiento y el funcionamiento del mundo en general.
Camille: Así que, después del entrenamiento previo, la IA es como un recién graduado universitario: inteligente y competentepero sin experiencia profesional específica.
Luc: Precisamente. Durante mucho tiempo, la etapa siguiente ha sido «el ajuste fino».
Camille: El ajuste fino… ¿Es aquí donde entra en juego el aprendizaje por transferencia? Ya he escuchado ese término.
Luc: Precisamente. El aprendizaje por transferencia es la clave. Vamos a verlo: no enseñarías matemáticas básicas a un brillante físico antes de que se enfrente a la mecánica cuántica. Transfiere sus habilidades matemáticas existentes. La IA hace lo mismo. Las lenguas son un excelente ejemplo.
Camille: ¿Qué quieres decir?
Luc: Puedes tomar un modelo experto en inglés y luego presentarle una cantidad mucho menor de texto en francés. Aprenderá el francés a una velocidad increíble.
Camille: ¿Por qué ya comprende los conceptos generales de gramática, sintaxis y estructura de oración gracias al inglés?
Luc: Exactamente. No necesita volver a aprender qué es un verbo. Simplemente aprende las palabras y las reglas del idioma francés, transfiriendo los conceptos subyacentes. Esta es toda la potencia de este enfoque.
Camille: En consecuencia, transfiere sus inmensos conocimientos generales adquiridos durante el pre-entrenamiento a la nueva tarea específica.
Luc: Exactamente, por eso puede convertirse en un experto de sus datos con una cantidad asombrosamente pequeña de nueva información. No comienza desde cero; se sustenta en fundamentos sumamente sólidos.
Camille: Es lógico. Pero, como discutimos en nuestro último episodio sobre los Transformadores, está surgiendo un nuevo enfoque más flexible, ¿no es así?
Luc: Sí, y esto se logra gracias al aumento masivo de la memoria a corto plazo de la IA, o "ventana de contexto". Este enfoque se llama aprendizaje en contexto, o ICL (Aprendizaje In-Contexto).
Camille: Entonces, en lugar de reentrenar la IA para convertirla en una especialista, simplemente le damos la información necesaria para cumplir con la tarea asignada.
Luc: Usted entiende todo. Es como contratar a un consultor brillante y, en lugar de enviarlo a un programa de capacitación de varios años, simplemente proporcionarle los documentos informativos exactos que necesita para el proyecto actual.
Camille: Es aquí donde actúa el concepto de «ancoraje», que consiste en vincular las respuestas de la IA a las informaciones específicas que vosotros proporcionáis.
Luc: Exactamente. Pero esto nos lleva a un punto crucial que a menudo se malinterpreta: la forma en que la IA "recuerda" esta información. Es la diferencia entre un conocimiento temporal y una competencia permanente. Camille: ¿A qué te refieres con eso?
Camille: "¿La diferencia entre estudiar para un examen y dominar realmente un tema?"
Luc: ¡Una analogía perfecta! El aprendizaje en contexto es como aprender de memoria. Los conocimientos que se proporcionan en el prompt son temporales. La IA los utiliza para esa conversación única, pero una vez que la conversación termina, estos conocimientos disappear.
Camille: Olvida todo.
Luc: Ella olvida todo. Por lo tanto, es una memoria de un solo uso. Si quiero que tenga conocimiento de las mismas informaciones mañana, tengo que proporcionarle nuevamente los documentos.
Camille: De acuerdo.
Luc: Esta es la realidad del ICL. Es increíblemente flexible, pero se basa en una memoria a corto plazo. El ajuste fino, por otro lado, busca crear una competencia permanente. Cuando ajustas un modelo, estás cambiando fundamentales su estructura interna. Los nuevos conocimientos se convierten en parte integral de su identidad. Camille: ¿Y eso cómo funciona? Luc: En contexto de aprendizaje, el "ajuste fino" se refiere a la forma en que las respuestas generadas por el modelo se alinean con tus expectativas y necesidades específicas. Es un proceso iterativo donde el modelo aprende a través de ejemplos y retroalimentación humana.
Camille: ¿Los conocimientos adquiridos durante el ajuste fino persisten en todas las conversaciones para siempre?
Luc: Sí. Es como aprender a montar en bicicleta. La competencia está anclada. No necesitas recordar las leyes del equilibrio cada vez que te sientas.
Camille: Luc, esto explica una experiencia muy común con los chatbots. Puedes tener una conversación larga y detallada, pero si abres una nueva ventana de conversación, la IA no tiene idea de lo que se ha dicho antes.
Luc: ¡Exacto! Este es el aprendizaje en contexto en acción. Todo el historial de su discusión en esta sesión constituye el contexto.
Camille: Entiendo.
Luc: Cuando abres una nueva ventana, partimos de un contexto vacío. La IA no ha "olvidado" en el sentido humano; su espacio de trabajo temporal simplemente se ha vaciado.
Camille: Pero ¿qué pasa con las nuevas funcionalidades como la "Memoria" que algunas IA están empezando a integrar? Da la impresión de que realmente comienzan a recordar cosas de una sesión a otra. Luc: ¡Exacto! Esta es la memoria a corto plazo que mencionamos antes, y es donde el aprendizaje en contexto entra en juego. La integración de la memoria permite a algunos modelos mantener información relevante entre conversaciones, lo que mejora su capacidad para recordar detalles y proporcionar respuestas más coherentes y contextualizadas. Sin embargo, es importante tener en cuenta que esta memoria es limitada y depende del contexto inmediato de la conversación. Camille: ¿Y qué pasa si el usuario no vuelve a interactuar con el chatbot durante mucho tiempo? ¿La información se desvanece con el tiempo o se puede recuperar en alguna forma? Luc: La información almacenada en la memoria temporal del modelo puede desvanecerse con el tiempo, especialmente si el usuario no proporciona nuevos datos para actualizarla. Sin embargo, en algunos casos, la información puede recuperarse si el contexto de la conversación anterior es suficiente para que el modelo recuerde los detalles relevantes.
Luc: Es una observación excelente y es crucial comprender cómo funciona. La IA no se refina constantemente con tus conversaciones. Sería increíblemente ineficiente.
Camille: Entonces, ¿es un truco?
Luc: Estas funciones de memorización son una forma ingeniosa de aprendizaje en contexto automatizado. Cuando se inicia una nueva conversación, el sistema busca rápidamente en sus intercambios anteriores las informaciones que parecen pertinentes para su nueva solicitud. Luego, inserta automáticamente estos extractos en el prompt, tras bambalinas.
Camille: Entonces, da la impresión de que la IA recuerda los detalles de mi proyecto, pero en realidad solo se le proporcionó una guía antes de empezar a conversar con usted.
Luc: Precisamente. El modelo en sí mismo no aprende ni evoluciona a partir de sus discusiones; simplemente utiliza un sistema más inteligente para recordar el contexto pasado.
Camille: Por lo tanto, la gran cuestión para cualquiera que utilice estas herramientas es ¿necesito un consultor temporal o un experto permanente?
Luc: Esta es la manera ideal de plantear el problema. Y sobre esta reflexión, es hora de concluir.
Camille: Gracias por escucharnos, ¡hasta la próxima entrega de Tech Éclair!
